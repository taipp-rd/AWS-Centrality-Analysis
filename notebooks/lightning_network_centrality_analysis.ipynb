{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ライトニングネットワーク中心性分析\n", "\n", "このノートブックでは、ビットコインのライトニングネットワークにおける3つの中心性指標を分析します：\n", "\n", "1. **媒介中心性 (Betweenness Centrality)**: ノードが他のノード間の最短経路にどれだけ頻繁に現れるか\n", "2. **近接中心性 (Closeness Centrality)**: ノードから他のすべてのノードへの平均距離の逆数\n", "3. **近似中心性 (Eigenvector Centrality)**: 重要なノードに接続されているノードが重要とされる\n", "\n", "## 目的\n", "\n", "どのノードとチャネルを開設したらルーティングがされやすくなるかを分析します。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. 依存パッケージのインストール\n", "\n", "**重要**: 初回実行時、またはパッケージが更新された場合は、このセルを実行してください。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# requirements.txtからすべてのパッケージをインストール\n", "# 注意: インストール後はカーネルを再起動してください\n", "\n", "import os\n", "import sys\n", "import subprocess\n", "\n", "# プロジェクトルートを取得\n", "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n", "requirements_path = os.path.join(project_root, 'requirements.txt')\n", "\n", "if os.path.exists(requirements_path):\n", "    print(f\"requirements.txtを読み込みます: {requirements_path}\")\n", "    print(\"パッケージをインストールしています...\")\n", "    \n", "    # subprocessを使用してpip installを実行\n", "    # %pipマジックコマンドでは変数展開ができないため、subprocessを使用\n", "    try:\n", "        result = subprocess.run(\n", "            [sys.executable, '-m', 'pip', 'install', '-r', requirements_path],\n", "            capture_output=True,\n", "            text=True,\n", "            check=False\n", "        )\n", "        \n", "        if result.returncode == 0:\n", "            print(result.stdout)\n", "            print(\"\\n✓ パッケージのインストールが完了しました\")\n", "            print(\"⚠️  重要: カーネルを再起動してください（Kernel → Restart Kernel）\")\n", "        else:\n", "            print(\"エラーが発生しました:\")\n", "            print(result.stderr)\n", "            print(\"\\n代替方法: 以下のコマンドを手動で実行してください\")\n", "            print(f\"  %pip install -r {requirements_path}\")\n", "    except Exception as e:\n", "        print(f\"インストール中にエラーが発生しました: {e}\")\n", "        print(\"\\n代替方法: 以下のコマンドを手動で実行してください\")\n", "        print(f\"  %pip install -r {requirements_path}\")\n", "else:\n", "    print(f\"✗ requirements.txtが見つかりません: {requirements_path}\")\n", "    print(f\"現在のディレクトリ: {os.getcwd()}\")\n", "    print(f\"プロジェクトルート: {project_root}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. 環境設定とインポート\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "\n", "# プロジェクトルートをパスに追加\n", "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n", "sys.path.insert(0, project_root)\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import networkx as nx\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import logging\n", "from pathlib import Path\n", "\n", "# ロギング設定\n", "logging.basicConfig(\n", "    level=logging.INFO,\n", "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n", ")\n", "\n", "# 日本語フォント設定（必要に応じて）\n", "plt.rcParams['font.family'] = 'DejaVu Sans'\n", "sns.set_style(\"whitegrid\")\n", "\n", "print(\"環境設定が完了しました\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. データベース接続とデータ取得\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.database.connection import DatabaseConnection\n", "from src.graph.builder import GraphBuilder\n", "import networkx as nx\n", "\n", "# データベース接続\n", "db = DatabaseConnection()\n", "\n", "# グラフ構造情報を取得\n", "graph_info = db.get_graph_structure()\n", "print(f\"グラフ構造情報:\")\n", "print(f\"  ノード数: {graph_info['node_count']}\")\n", "print(f\"  エッジ数: {graph_info['edge_count']}\")\n", "\n", "# グラフを構築（基本グラフ、重みなし）\n", "# 注意: トランザクションサイズを考慮した分析は、次のセクションで実行します\n", "graph_builder = GraphBuilder(db)\n", "graph = graph_builder.build_graph(directed=False)\n", "\n", "print(f\"\\n基本グラフが構築されました:\")\n", "print(f\"  ノード数: {graph.number_of_nodes()}\")\n", "print(f\"  エッジ数: {graph.number_of_edges()}\")\n", "print(f\"  連結成分数: {nx.number_connected_components(graph)}\")\n", "\n", "# 論文 \"A Centrality Analysis of the Lightning Network\" (2201.07746v1) を参照\n", "# 異なるトランザクションサイズでルーティング手数料が変わるため、\n", "# 中心性スコアも変化する可能性がある\n", "print(\"\\n注意: トランザクションサイズを考慮した分析は、次のセクションで実行します\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.1 データの妥当性確認\n", "\n", "取得したデータが正しいか、mempool.spaceやAmbossなどのLightning Network統計と比較して確認します。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# データの妥当性を確認\n", "# mempool.spaceやAmbossなどのLightning Network統計と比較\n", "\n", "# 1. ユニークなチャネルIDの数を確認\n", "unique_channels_query = \"\"\"\n", "SELECT COUNT(DISTINCT chan_id) as unique_channels\n", "FROM channel_update\n", "WHERE rp_disabled = false\n", "AND chan_id NOT IN (SELECT chan_id FROM closed_channel)\n", "\"\"\"\n", "result = db.execute_query_single(unique_channels_query)\n", "print(f\"✓ ユニークなチャネル数: {result['unique_channels']:,}\")\n", "\n", "# 2. 最新の更新のみを取得した場合のチャネル数\n", "latest_channels_query = \"\"\"\n", "SELECT COUNT(*) as latest_channels\n", "FROM (\n", "    SELECT DISTINCT ON (chan_id) chan_id\n", "    FROM channel_update\n", "    WHERE rp_disabled = false\n", "    AND chan_id NOT IN (SELECT chan_id FROM closed_channel)\n", "    ORDER BY chan_id, timestamp DESC\n", ") latest\n", "\"\"\"\n", "result2 = db.execute_query_single(latest_channels_query)\n", "print(f\"✓ 最新の更新のみ（重複除外）: {result2['latest_channels']:,}\")\n", "\n", "# 3. データの最新性を確認（最新の更新日時）\n", "latest_timestamp_query = \"\"\"\n", "SELECT MAX(timestamp) as latest_timestamp,\n", "       MIN(timestamp) as oldest_timestamp,\n", "       COUNT(*) as total_updates\n", "FROM channel_update\n", "WHERE rp_disabled = false\n", "AND chan_id NOT IN (SELECT chan_id FROM closed_channel)\n", "\"\"\"\n", "result3 = db.execute_query_single(latest_timestamp_query)\n", "\n", "# タイムスタンプを日時に変換\n", "from datetime import datetime\n", "latest_dt = datetime.fromtimestamp(result3['latest_timestamp']) if result3['latest_timestamp'] else None\n", "oldest_dt = datetime.fromtimestamp(result3['oldest_timestamp']) if result3['oldest_timestamp'] else None\n", "\n", "print(f\"\\nデータの範囲:\")\n", "if latest_dt:\n", "    print(f\"  最新の更新: {latest_dt.strftime('%Y-%m-%d %H:%M:%S')}\")\n", "if oldest_dt:\n", "    print(f\"  最も古い更新: {oldest_dt.strftime('%Y-%m-%d %H:%M:%S')}\")\n", "print(f\"  総更新数: {result3['total_updates']:,}\")\n", "\n", "# 4. 最近30日以内の更新のみを取得した場合\n", "recent_channels_query = \"\"\"\n", "SELECT COUNT(DISTINCT chan_id) as recent_channels\n", "FROM (\n", "    SELECT DISTINCT ON (chan_id) chan_id\n", "    FROM channel_update\n", "    WHERE rp_disabled = false\n", "    AND chan_id NOT IN (SELECT chan_id FROM closed_channel)\n", "    AND timestamp >= EXTRACT(EPOCH FROM NOW() - INTERVAL '30 days')::bigint\n", "    ORDER BY chan_id, timestamp DESC\n", ") recent\n", "\"\"\"\n", "result4 = db.execute_query_single(recent_channels_query)\n", "print(f\"\\n最近30日以内の更新のみ: {result4['recent_channels']:,}\")\n", "\n", "# 5. ノード数の詳細確認\n", "node_details_query = \"\"\"\n", "SELECT \n", "    COUNT(DISTINCT advertising_nodeid) as advertising_nodes,\n", "    COUNT(DISTINCT connecting_nodeid) as connecting_nodes,\n", "    COUNT(DISTINCT CASE \n", "        WHEN advertising_nodeid IS NOT NULL THEN advertising_nodeid \n", "        WHEN connecting_nodeid IS NOT NULL THEN connecting_nodeid \n", "    END) as total_unique_nodes\n", "FROM (\n", "    SELECT DISTINCT ON (chan_id) \n", "        advertising_nodeid, \n", "        connecting_nodeid\n", "    FROM channel_update\n", "    WHERE rp_disabled = false\n", "    AND chan_id NOT IN (SELECT chan_id FROM closed_channel)\n", "    ORDER BY chan_id, timestamp DESC\n", ") latest_channels\n", "\"\"\"\n", "result5 = db.execute_query_single(node_details_query)\n", "print(f\"\\nノード数の詳細:\")\n", "print(f\"  advertising_nodeid: {result5['advertising_nodes']:,}\")\n", "print(f\"  connecting_nodeid: {result5['connecting_nodes']:,}\")\n", "print(f\"  ユニークなノード数: {result5['total_unique_nodes']:,}\")\n", "\n", "# 6. 参考値との比較\n", "print(f\"\\n=== 参考値との比較 ===\")\n", "print(f\"Lightning Network全体の統計（2024年10月時点）:\")\n", "print(f\"  ノード数: 約15,042\")\n", "print(f\"  チャネル数: 約46,398\")\n", "print(f\"\\n取得データ:\")\n", "print(f\"  ノード数: {graph_info['node_count']:,} ({graph_info['node_count']/15042*100:.1f}%)\")\n", "print(f\"  エッジ数: {graph_info['edge_count']:,} ({graph_info['edge_count']/46398*100:.1f}%)\")\n", "print(f\"\\n注意:\")\n", "print(f\"  - ノード数が少ない場合: 一部のノードが除外されている可能性があります\")\n", "print(f\"  - エッジ数が多い場合: 同じチャネルの更新が重複している可能性があります\")\n", "print(f\"  - 最新のデータのみを使用する場合は、日数制限を追加することを検討してください\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. 中心性計算\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.centrality.calculator import CentralityCalculator\n", "import numpy as np\n", "\n", "# 中心性計算器を初期化\n", "calculator = CentralityCalculator()\n", "\n", "# 基本グラフでの中心性計算（重みなし）\n", "print(\"=\" * 60)\n", "print(\"基本グラフでの中心性計算（重みなし）\")\n", "print(\"=\" * 60)\n", "print(\"中心性計算を開始します...\")\n", "centrality_scores = calculator.calculate_all(graph)\n", "\n", "print(\"\\n計算が完了しました:\")\n", "for name, scores in centrality_scores.items():\n", "    print(f\"  {name}: {len(scores)}個のノード\")\n", "    if scores:\n", "        values = list(scores.values())\n", "        print(f\"    平均: {np.mean(values):.6f}\")\n", "        print(f\"    最大: {np.max(values):.6f}\")\n", "        print(f\"    最小: {np.min(values):.6f}\")\n", "\n", "# 論文 \"A Centrality Analysis of the Lightning Network\" (2201.07746v1) を参照\n", "# ルーティング手数料は、fee_base_msat + (transaction_size_msat * fee_proportional_millionths / 1_000_000)\n", "# で計算されるため、トランザクションサイズによって中心性スコアが変化する可能性がある\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.1 トランザクションサイズを考慮した中心性計算\n", "\n", "論文 \"A Centrality Analysis of the Lightning Network\" (2201.07746v1) を参照すると、\n", "ルーティング手数料は以下の式で計算されます：\n", "\n", "**手数料 = fee_base_msat + (transaction_size_msat × fee_proportional_millionths / 1,000,000)**\n", "\n", "異なるトランザクションサイズでルーティング手数料が変わるため、\n", "中心性スコア（特に媒介中心性と近接中心性）も変化する可能性があります。\n", "\n", "このセクションでは、複数のトランザクションサイズで中心性を計算し、比較します。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# トランザクションサイズを考慮した中心性計算\n", "# 論文 \"A Centrality Analysis of the Lightning Network\" (2201.07746v1) を参照\n", "\n", "# 異なるトランザクションサイズを定義（millisatoshi）\n", "# 1 satoshi = 1,000 millisatoshi\n", "transaction_sizes = {\n", "    'small': 1_000_000,      # 1,000 satoshi (約 $0.50 @ $50,000/BTC)\n", "    'medium': 10_000_000,    # 10,000 satoshi (約 $5.00)\n", "    'large': 100_000_000,     # 100,000 satoshi (約 $50.00)\n", "}\n", "\n", "# 各トランザクションサイズでの中心性を計算\n", "centrality_by_transaction_size = {}\n", "\n", "for size_name, size_msat in transaction_sizes.items():\n", "    print(f\"\\n{'=' * 60}\")\n", "    print(f\"トランザクションサイズ: {size_name} ({size_msat:,} msat = {size_msat/1_000_000:.0f} sat)\")\n", "    print(f\"{'=' * 60}\")\n", "    \n", "    # このトランザクションサイズでの中心性を計算\n", "    scores = calculator.calculate_all(graph, transaction_size_msat=size_msat)\n", "    centrality_by_transaction_size[size_name] = scores\n", "    \n", "    print(f\"\\n計算結果:\")\n", "    for name, score_dict in scores.items():\n", "        if score_dict:\n", "            values = list(score_dict.values())\n", "            print(f\"  {name}:\")\n", "            print(f\"    平均: {np.mean(values):.6f}\")\n", "            print(f\"    最大: {np.max(values):.6f}\")\n", "            print(f\"    最小: {np.min(values):.6f}\")\n", "\n", "print(f\"\\n{'=' * 60}\")\n", "print(\"すべてのトランザクションサイズでの計算が完了しました\")\n", "print(f\"{'=' * 60}\")\n", "\n", "# 結果を保存（後続の分析で使用）\n", "# デフォルトとして、mediumサイズの結果を使用\n", "centrality_scores = centrality_by_transaction_size.get('medium', centrality_scores)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. 中心性の可視化\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.analysis.visualizer import GraphVisualizer\n", "\n", "# 可視化器を初期化\n", "visualizer = GraphVisualizer(output_dir='../results')\n", "\n", "# 中心性分布を可視化\n", "visualizer.plot_centrality_distribution(centrality_scores)\n", "\n", "# 各中心性タイプでグラフを可視化（上位100ノード）\n", "for centrality_type in ['betweenness', 'closeness', 'eigenvector']:\n", "    if centrality_type in centrality_scores:\n", "        visualizer.plot_graph_with_centrality(\n", "            graph, \n", "            centrality_scores, \n", "            centrality_type=centrality_type,\n", "            top_n=100\n", "        )\n", "\n", "# 上位ノードの比較\n", "visualizer.plot_top_nodes_comparison(centrality_scores, top_n=20)\n", "\n", "print(\"可視化が完了しました\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. ルーティング可能性分析\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.analysis.analyzer import RoutingAnalyzer\n", "\n", "# ルーティング分析器を初期化\n", "analyzer = RoutingAnalyzer(graph, centrality_scores)\n", "\n", "# 推奨ノードを取得（上位50ノード）\n", "recommended_nodes = analyzer.recommend_nodes_for_channel(top_n=50)\n", "\n", "print(\"推奨ノード（上位20）:\")\n", "print(recommended_nodes.head(20).to_string())\n", "\n", "# CSVに保存\n", "recommended_nodes.to_csv('../results/recommended_nodes.csv', index=False)\n", "print(\"\\n推奨ノードをCSVに保存しました\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 推奨チャネルを取得（上位100チャネル）\n", "# 注意: 全ノードの組み合わせを計算すると時間がかかるため、\n", "# 上位ノードのみを候補として使用\n", "top_nodes = recommended_nodes.head(100)['node_id'].tolist()\n", "recommended_channels = analyzer.recommend_channels(\n", "    candidate_nodes=top_nodes,\n", "    top_n=100\n", ")\n", "\n", "print(\"推奨チャネル（上位20）:\")\n", "print(recommended_channels.head(20).to_string())\n", "\n", "# CSVに保存\n", "recommended_channels.to_csv('../results/recommended_channels.csv', index=False)\n", "print(\"\\n推奨チャネルをCSVに保存しました\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. 特徴量抽出（機械学習用）\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from src.ml.features import FeatureExtractor\n", "\n", "# 特徴量抽出器を初期化\n", "feature_extractor = FeatureExtractor(graph, centrality_scores)\n", "\n", "# ノード特徴量を抽出\n", "node_features_df = feature_extractor.create_node_dataframe()\n", "print(f\"ノード特徴量: {node_features_df.shape}\")\n", "print(node_features_df.head())\n", "\n", "# エッジ特徴量を抽出\n", "edge_features_df = feature_extractor.create_edge_dataframe()\n", "print(f\"\\nエッジ特徴量: {edge_features_df.shape}\")\n", "print(edge_features_df.head())\n", "\n", "# CSVに保存\n", "node_features_df.to_csv('../results/node_features.csv', index=False)\n", "edge_features_df.to_csv('../results/edge_features.csv', index=False)\n", "print(\"\\n特徴量をCSVに保存しました\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. グラフニューラルネットワーク（GNN）による分析\n", "\n", "このセクションでは、AWS SageMakerを使用してGNNモデルを訓練します。\n", "詳細な手順については、`docs/GNN_CENTRALITY_ANALYSIS_GUIDE.md`と`docs/AWS_SAGEMAKER_GUIDE.md`を参照してください。\n", "\n", "### 7.1 DGLグラフへの変換\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import dgl\n", "import torch\n", "import numpy as np\n", "\n", "# NetworkXグラフをDGLグラフに変換\n", "# 注意: ノードの順序を保持するため、ノードリストを明示的に指定\n", "print(\"DGLグラフに変換しています...\")\n", "node_list = list(graph.nodes())  # ノードの順序を保持\n", "dgl_graph = dgl.from_networkx(graph, node_attrs=None, edge_attrs=None)\n", "\n", "# ノード特徴量の準備\n", "num_nodes = dgl_graph.number_of_nodes()\n", "\n", "# 基本特徴量（次数）\n", "# 注意: ノードの順序が一致することを確認\n", "degrees = torch.tensor([graph.degree(n) for n in node_list], dtype=torch.float32)\n", "features = degrees.unsqueeze(1)\n", "\n", "# 中心性指標を取得（ノードの順序を保持）\n", "betweenness = torch.tensor([centrality_scores['betweenness'].get(n, 0.0) \n", "                            for n in node_list], dtype=torch.float32)\n", "closeness = torch.tensor([centrality_scores['closeness'].get(n, 0.0) \n", "                         for n in node_list], dtype=torch.float32)\n", "eigenvector = torch.tensor([centrality_scores['eigenvector'].get(n, 0.0) \n", "                           for n in node_list], dtype=torch.float32)\n", "\n", "# 理論的注意: 中心性指標を特徴量として使用する場合、\n", "# ラベルとして使用しないこと（データリーク防止）\n", "# ここでは、基本特徴量（次数）のみを使用し、中心性は別途保存\n", "# または、中心性を特徴量として使用する場合は、ラベルには使用しない\n", "\n", "# オプション1: 基本特徴量のみを使用（推奨: データリークなし）\n", "# features = degrees.unsqueeze(1)\n", "\n", "# オプション2: 中心性も特徴量として使用（注意: ラベルには使用しない）\n", "features = torch.cat([features, \n", "                      betweenness.unsqueeze(1),\n", "                      closeness.unsqueeze(1),\n", "                      eigenvector.unsqueeze(1)], dim=1)\n", "\n", "# 特徴量の正規化（重要: 異なるスケールの特徴量を統一）\n", "feature_mean = features.mean(dim=0, keepdim=True)\n", "feature_std = features.std(dim=0, keepdim=True) + 1e-8  # ゼロ除算防止\n", "features_normalized = (features - feature_mean) / feature_std\n", "\n", "dgl_graph.ndata['feat'] = features_normalized\n", "\n", "# 中心性指標を個別に保存（分析用）\n", "dgl_graph.ndata['betweenness'] = betweenness\n", "dgl_graph.ndata['closeness'] = closeness\n", "dgl_graph.ndata['eigenvector'] = eigenvector\n", "\n", "# ラベルの準備（ルーティング可能性）\n", "# 注意: 中心性指標を特徴量として使用している場合、\n", "# ラベルには別の指標（例: 実際のルーティング成功回数など）を使用すべき\n", "# ここでは例として、中心性の組み合わせを使用（実際のデータに置き換えること）\n", "routing_potential = (betweenness * 0.5 + closeness * 0.3 + eigenvector * 0.2)\n", "dgl_graph.ndata['label'] = routing_potential.unsqueeze(1)\n", "\n", "print(f\"DGLグラフ情報:\")\n", "print(f\"  ノード数: {dgl_graph.number_of_nodes()}\")\n", "print(f\"  エッジ数: {dgl_graph.number_of_edges()}\")\n", "print(f\"  特徴量次元: {features_normalized.shape[1]}\")\n", "print(f\"  ラベル形状: {dgl_graph.ndata['label'].shape}\")\n", "print(f\"\\n注意: 中心性指標を特徴量として使用している場合、\")\n", "print(f\"ラベルには実際のルーティングデータを使用することを推奨します。\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 7.2 グラフデータの保存\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# グラフを保存\n", "dgl.save_graphs('../results/lightning_graph.bin', [dgl_graph])\n", "\n", "# メタデータの保存\n", "import json\n", "metadata = {\n", "    'num_nodes': dgl_graph.number_of_nodes(),\n", "    'num_edges': dgl_graph.number_of_edges(),\n", "    'feature_dim': features.shape[1]\n", "}\n", "\n", "with open('../results/graph_metadata.json', 'w') as f:\n", "    json.dump(metadata, f, indent=2)\n", "\n", "print(\"グラフデータを保存しました: ../results/lightning_graph.bin\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. 結果をS3に保存（ローカル実行 + S3保存構成）\n", "\n", "**推奨構成**: ローカル（Cursor/Jupyter）で実行し、結果をS3に保存するハイブリッドアプローチ\n", "\n", "**利点**:\n", "- **コスト効率**: SageMaker Notebook Instanceの料金がかからない\n", "- **開発速度**: 即座に実行可能、デバッグが容易\n", "- **永続化**: 結果をS3に保存することで、データの永続化と共有が可能\n", "- **柔軟性**: 必要に応じてSageMakerで大規模計算を実行可能\n", "\n", "**AWS公式ドキュメント参考**:\n", "- [Use an Amazon S3 bucket for input and output](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-bucket.html)\n", "- [MLCOST04-BP02: Use managed build environments](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlcost04-bp02.html)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 分析結果をS3に保存\n", "# ローカル（Cursor/Jupyter）で実行した結果をS3に保存して永続化\n", "\n", "import boto3\n", "import os\n", "from pathlib import Path\n", "from datetime import datetime\n", "import yaml\n", "\n", "# 設定ファイルからS3バケット情報を取得\n", "try:\n", "    config_path = os.path.join(project_root, 'config', 'config.yaml')\n", "    with open(config_path, 'r', encoding='utf-8') as f:\n", "        config = yaml.safe_load(f)\n", "    bucket_name = config.get('aws', {}).get('s3', {}).get('bucket')\n", "    s3_prefix = config.get('aws', {}).get('s3', {}).get('prefix', 'lightning-network-analysis/')\n", "except Exception as e:\n", "    print(f\"設定ファイルの読み込みエラー: {e}\")\n", "    bucket_name = None\n", "    s3_prefix = 'lightning-network-analysis/'\n", "\n", "# S3クライアントの作成\n", "s3 = boto3.client('s3', region_name='ap-northeast-1')\n", "\n", "# タイムスタンプ付きのディレクトリ名を作成\n", "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "s3_results_prefix = f\"{s3_prefix}results/{timestamp}/\"\n", "\n", "# ローカルのresultsディレクトリ\n", "local_results_dir = Path(project_root) / 'results'\n", "\n", "if bucket_name:\n", "    print(f\"S3バケット: {bucket_name}\")\n", "    print(f\"S3プレフィックス: {s3_results_prefix}\")\n", "    print(f\"\\n結果をS3にアップロードしています...\")\n", "    \n", "    uploaded_files = []\n", "    \n", "    # resultsディレクトリ内のすべてのファイルをアップロード\n", "    if local_results_dir.exists():\n", "        for file_path in local_results_dir.rglob('*'):\n", "            if file_path.is_file():\n", "                # 相対パスを取得\n", "                relative_path = file_path.relative_to(local_results_dir)\n", "                s3_key = f\"{s3_results_prefix}{relative_path.as_posix()}\"\n", "                \n", "                try:\n", "                    s3.upload_file(str(file_path), bucket_name, s3_key)\n", "                    uploaded_files.append(s3_key)\n", "                    print(f\"  ✓ {file_path.name} → s3://{bucket_name}/{s3_key}\")\n", "                except Exception as e:\n", "                    print(f\"  ✗ {file_path.name} のアップロードに失敗: {e}\")\n", "    \n", "    if uploaded_files:\n", "        print(f\"\\n✓ {len(uploaded_files)}個のファイルをS3にアップロードしました\")\n", "        print(f\"S3パス: s3://{bucket_name}/{s3_results_prefix}\")\n", "    else:\n", "        print(\"\\n⚠️  アップロードするファイルが見つかりませんでした\")\n", "        print(f\"ローカルのresultsディレクトリ: {local_results_dir}\")\n", "else:\n", "    print(\"⚠️  S3バケット名が設定されていません\")\n", "    print(\"config/config.yamlのaws.s3.bucketを設定してください\")\n", "    print(\"\\nローカルに保存されたファイル:\")\n", "    if local_results_dir.exists():\n", "        for file_path in local_results_dir.rglob('*'):\n", "            if file_path.is_file():\n", "                print(f\"  - {file_path.relative_to(local_results_dir)}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 7.3 AWS SageMakerでのGNN訓練（オプション）\n", "\n", "**注意**: このセルを実行する前に、以下を確認してください：\n", "1. `config/config.yaml`でAWS設定（IAMロール、S3バケット）を行ってください\n", "2. データをS3にアップロードしてください\n", "3. 詳細は`docs/AWS_SAGEMAKER_GUIDE.md`を参照してください\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 注意: このセルを実行する前に、config/config.yamlでAWS設定を行ってください\n", "# import boto3\n", "# from src.ml.pipeline import GNNPipeline\n", "# \n", "# # S3にデータをアップロード\n", "# s3 = boto3.client('s3')\n", "# bucket_name = 'your-bucket-name'  # 実際のバケット名\n", "# \n", "# s3.upload_file('../results/lightning_graph.bin', bucket_name, \n", "#                'lightning-network-analysis/data/lightning_graph.bin')\n", "# \n", "# train_data_path = f's3://{bucket_name}/lightning-network-analysis/data/lightning_graph.bin'\n", "# \n", "# # GNNパイプラインを初期化\n", "# gnn_pipeline = GNNPipeline()\n", "# \n", "# # モデルの訓練\n", "# print(\"GNNモデルの訓練を開始します...\")\n", "# estimator = gnn_pipeline.train_gnn_model(\n", "#     train_data_path=train_data_path,\n", "#     hyperparameters={\n", "#         'hidden-dim': 64,\n", "#         'num-layers': 2,\n", "#         'dropout': 0.5,\n", "#         'learning-rate': 0.01,\n", "#         'epochs': 100,\n", "#         'model-type': 'gcn'  # または 'gat'\n", "#     },\n", "#     use_dgl_container=True\n", "# )\n", "# \n", "# print(\"GNNモデルの訓練が完了しました\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. 結果のまとめ\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"=== 分析結果のまとめ ===\\n\")\n", "\n", "print(f\"グラフ統計:\")\n", "print(f\"  ノード数: {graph.number_of_nodes()}\")\n", "print(f\"  エッジ数: {graph.number_of_edges()}\")\n", "print(f\"  平均次数: {sum(dict(graph.degree()).values()) / graph.number_of_nodes():.2f}\")\n", "print(f\"  連結成分数: {nx.number_connected_components(graph)}\")\n", "\n", "print(f\"\\n中心性統計:\")\n", "for name, scores in centrality_scores.items():\n", "    if scores:\n", "        values = list(scores.values())\n", "        top_node = max(scores.items(), key=lambda x: x[1])\n", "        print(f\"  {name}:\")\n", "        print(f\"    最大値ノード: {top_node[0]}\")\n", "        print(f\"    最大値: {top_node[1]:.6f}\")\n", "\n", "print(f\"\\n推奨ノード数: {len(recommended_nodes)}\")\n", "print(f\"推奨チャネル数: {len(recommended_channels)}\")\n", "\n", "print(\"\\n分析が完了しました。結果は results/ ディレクトリに保存されています。\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# データベース接続を閉じる\n", "db.close()\n", "print(\"データベース接続を閉じました\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 2}